<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FSSR6942QL"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FSSR6942QL');
    </script>

    <title>Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting </title>
    <meta name="description" content="Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting">
    <meta name="viewport" content="width=device-width, initial-scale=1">

     <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/styles/default.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bulma.min.css">
    <link rel="stylesheet" href="css/fontawesome.all.min.css">
    <link rel="stylesheet" href="css/bulma-carousel.min.css">
    <link rel="stylesheet" href="css/bulma-slider.min.css">
    <link rel="stylesheet" href="css/bulma-myscope.css">
    <!--<link rel="icon" href="img/hri_transparent.png">-->

   <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.6.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/bulma-carousel.min.js"></script>
    <script src="js/bulma-slider.min.js"></script>
    <script defer src="js/fontawesome.all.min.js"></script>
    <script src="js/index.js"></script>
    <script src="js/app.js"></script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>

    <style>
    .method-image {
        border-radius: 10px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        margin-bottom: 1rem; /* Space between image and caption */
    }
    
    /* NEW: A WIDER container for the safety degradation image */
    .figure-container--wide {
        max-width: 700px; /* Adjust this value to control the size */
        margin: 0 auto;   /* This centers the container */
    }

    /* NEW: A NARROWER container for the logprobs image to make it appear taller/larger */
    .figure-container--narrow {
        max-width: 400px; /* Adjust this value to control the size */
        margin: 0 auto;   /* This centers the container */
    }
</style>

    <style>
    /* Creates a consistent container for each logo */
    .logo-item {
        max-width: 220px;  /* Adjust this value to control the max width of each logo */
        max-height: 60px; /* This is the max height we want */
        display: flex;
        justify-content: center;
        align-items: center;
    }

    /* NEW: A modifier class to make a specific logo smaller */
    .logo-item--smaller {
        max-height: 30px; /* Adjust this value down until it looks right */
    }

    /* Makes the image inside the container fit properly */
    .logo-item img {
        max-width: 100%;
        max-height: 100%;
        object-fit: contain; /* Ensures the image scales without being distorted */
    }
    </style>
</head>
<body>
    <div class="container" id="main">
        <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
                Actions as Language: </br> Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting
            </h1>
        </div>
        
        <div class="row has-text-centered">
            <div class="is-size-5 publication-authors">
                <span class="author-block">
                    <a href="https://aasherh.github.io/">Asher J. Hancock,</a>
                </span>
                <span class="author-block">
                    <a href="https://xindiwu.github.io/"> Xindi Wu,</a>
                </span>
                <span class="author-block">
                    <a href="https://lihzha.github.io/"> Lihan Zha,</a>
                </span>
                <span class="author-block">
                    <a href="https://www.cs.princeton.edu/~olgarus/"> Olga Russakovsky</a>
                </span>
                <span class="author-block">
                    <a href="https://irom-lab.princeton.edu/majumdar/"> Anirudha Majumdar</a>
                </span>
            </div>

            <!--
            <div class="column has-text-centered">
                <div class="publication-links">
                  
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2410.01971"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
            
                  
                  <span class="link-block">
                    <a href="https://github.com/irom-lab/byovla"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  
                  
                  <span class="link-block">
                    <a href="https://youtu.be/9PXC1tnD91g"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
  
                </div>
  
              </div>
            -->
        </div>

        <br>
        <br>
        <!-- ================================================================================= -->
        <!-- LOGO BAR SECTION (CORRECTED FLEXBOX METHOD) -->
        <!-- ================================================================================= -->
        <div class="row">
            <div class="col-md-12">
                <div class="d-flex justify-content-center align-items-center">

                    <!-- Logo 1: IROM Lab -->
                    <div class="mx-4 logo-item"> <!-- Added 'logo-item' class -->
                        <a href="https://irom-lab.princeton.edu/">
                            <!-- Removed height attribute from img tag -->
                            <img src="img/irom_lab.png" alt="Intelligent Robot Motion Lab Logo">
                        </a>
                    </div>

                    <!-- Logo 3: Your Project/Lab -->
                    <div class="mx-4 logo-item--smaller"> <!-- Added 'logo-item' class -->
                        <a href="#"> 
                            <img src="img/logowithtext.png" alt="Princeton Visual AI Lab Logo">
                        </a>
                    </div>

                </div>
            </div>
        </div>
        <br>

        <!--<Add a large space between logos and the image below>-->
        <br>
        <br>
        


        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <p style="text-align:center;">
                    <image src="img/vlmvla_anchor_main_idood.png" width="100%"></image>
                </p>
                <br>
                <h2 class="title is-3" style="text-align: center;">TLDR</h2>
                <p class="text-justify">
                    We introduce <b>VLM2VLA</b>, a VLA model training paradigm that represents low-level robot actions as natural language to better align the robot fine-tuning data with the base VLM's representation space. 
                    This alignment, combined with parameter-efficient fine-tuning, better preserves the VLM's foundational reasoning during VLA fine-tuning. 
                    Through extensive experimentation, <b>VLM2VLA</b> enables strong VQA performance and zero-shot generalization to new scenarios.</br>
                </p>
            </div>
        </div>
        <br>

        <!-- <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
                <h2 class="title is-3">Approach</h2>
                <p style="text-align:center;">
                    <image src="img/vlmvla_anchor_main_idood.png" width="100%"></image>
                </p>
                <br>
                <p class="text-justify">
                    BYOVLA is predicated on three simple steps applied to a VLA's input image: 1) determine task-irrelevant regions, 2) quantify sensitivity by perturbing regions, and 3) transform the image. 
                    Task-irrelevant regions are determined by a vision-language model (VLM). If the VLA is sensitive to a task-irrelevant region, BYOVLA transforms the image such that the model is no longer sensitive by simply removing objects and recoloring backgrounds distractions. 
                </p>
            </div>
        </div> -->
        <br>

        <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
            <p style="text-align:center;">
            </p>
            <h2 class="title is-3" style="text-align: center;">Abstract</h2>
                <p class="text-justify" style="text-align: center; margin-bottom: 1.5em;">
                    Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM's foundational reasoning and multimodal understanding, which hinders generalization to novel scenarios, instruction following, and semantic understanding.
                    We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM's internet-scale pretraining corpus and the robotics fine-tuning data.
                    Inspired by this observation, we introduce <b>VLM2VLA</b>: a VLA training paradigm whose core idea is to <i>represent low-level actions using natural language</i> by relabeling robot demonstration data. 
                    This methodology better aligns the fine-tuning data to the base model's representation space, thereby mitigating the distribution mismatch while enabling effective robotic control. 
                    As a result, the VLM can be fine-tuned on robot teleoperation data using a policy architecture that makes no modifications to the underlying VLM and without expensive co-training on internet-scale VLM datasets. 
                    Through extensive Visual Question Answering (VQA) studies and real-world robotics experiments, we demonstrate that <b>VLM2VLA</b> preserves the VLM's core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following.<br>
                </p>
            </div>
        </div>
        <br>

        <!-- ================================================================================= -->
<!-- START CORE IDEA / METHODOLOGY SECTION (FINAL LAYOUT) -->
<!-- ================================================================================= -->

<style>
    .method-image {
        border-radius: 10px;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        margin-bottom: 1rem; /* Space between image and caption */
    }
    
    /* A WIDER container for the safety degradation image */
    .figure-container--wide {
        max-width: 700px; /* Adjust this value to control the size */
        margin: 0 auto;   /* This centers the container */
    }

    /* A NARROWER container for the logprobs image to make it appear taller/larger */
    .figure-container--narrow {
        max-width: 400px; /* Adjust this value to control the size */
        margin: 0 auto;   /* This centers the container */
    }
</style>

<section class="section">
    <div class="container is-max-desktop">

        <!-- Main Section Title and Summary Paragraph -->
        <div class="row justify-content-md-center">
            <div class="col-md-12 col-lg-10">
                <h2 class="title is-3" style="text-align: center;">The Core Idea: Aligning Actions with Language</h2>
                <p class="text-justify">
                    Fine-tuning VLMs into robotic policies (VLAs) often leads to a critical problem: the model forgets its vast world knowledge. We argue this happens because of a fundamental "distribution mismatch" between the VLM's original internet-scale pretraining and the specialized, low-level action space of robotics. Our key insight is to address this problem at the data level: by <strong>re-representing robot actions as natural language</strong>, we align the fine-tuning data directly with the VLM's pretrained knowledge. This data transformation, combined with parameter-efficient fine-tuning strategies, such as Low-Rank Adaptation (LoRA), helps preserve the model's reasoning capabilities without expensive co-training or complex architectural changes.
                </p>
            </div>
        </div>
        <br>

        <!-- ROW 1 FOR THE LOGPROBS IMAGE (WITH SIMPLE SCALING) -->
        <div class="row justify-content-center text-center">
            <div class="col-md-8">
                <!-- The container div is no longer needed for sizing -->
                <div>
                    <!-- === CHANGED HERE: Added an inline style to scale the image === -->
                    <img src="img/logprobs_copy.png" class="img-fluid method-image" alt="Graph showing higher log probabilities for language-based actions." style="transform: scale(1);">
                </div>
                <p class="text-justify" style="margin-top: 2rem;"> <!-- Added margin-top to create space after the scaled image -->
                    <em><strong>Before fine-tuning</strong>, the base VLM (Gemma-3-12B-IT) assigns much higher probabilities to our natural language action descriptions than to standard, tokenized robot actions. This shows our data is better aligned with the model's original knowledge.</em>
                </p>
            </div>
        </div>
                
        <br><br> <!-- Extra space between the two figures -->

        <!-- ROW 2 FOR THE SAFETY IMAGE (WIDE CONTAINER) -->
        <div class="row justify-content-center text-center">
            <div class="col-md-8">
                <!-- Using the WIDE container here -->
                <div class="figure-container--wide">
                    <img src="img/Safety_Knowledge_Degradation.png" class="img-fluid method-image" alt="Comparison showing our method preserves world knowledge and reasoning capabilities at runtime.">
                </div>
                <p class="text-justify">
                    <em><strong>After fine-tuning</strong>, Traditional VLA training procedures often overfit to the robot training data, sacrificing their original reasoning capabilities for low-level action prediction (center). In contrast, <b> VLM2VLA </b> preserves the world understanding of the pretrained VLM (left), allowing the model to reason about potential safety risks instead of just motor commands.</em>
                </p>
            </div>
        </div>

    </div>
</section>
<!-- ================================================================================= -->
<!-- END CORE IDEA / METHODOLOGY SECTION -->
<!-- ================================================================================= -->




        <!-- ================================================================================= -->
        <!-- START MULTIMODAL UNDERSTANDING EVALUATION SECTION -->
        <!-- ================================================================================= -->
        <section class="section">
            <div class="container is-max-desktop">

                <!-- Section Title and Description (Full Width) -->
                <div class="row justify-content-md-center">
                    <div class="col-md-12 col-lg-10">
                        <h2 class="title is-3" style="text-align: center;">Multimodal Understanding Evaluation</h2>
                        <p class="text-justify" style="text-align: center; margin-bottom: 1.5em;">
                            Comparison of VLMs and VLAs across multimodal understanding benchmarks. Our method preserves strong performance across diverse multimodal understanding tasks while maintaining robotic capabilities. The best and second best results for each benchmark are shown in <strong>bold</strong> and <u>underlined</u>, respectively.
                        </p>
                    </div>
                </div>
                <br>

                <!-- NEW Row for Side-by-Side Content -->
                <div class="row justify-content-md-center">
                    
                    <!-- Left Column (8/12 width): The Table -->
                    <div class="col-md-8">
                        <div style="overflow-x: auto;">
                            <table class="table table-bordered text-center">
                                <thead>
                                    <tr>
                                        <th>Method</th>
                                        <th>#Params</th>
                                        <th>MMMU</th>
                                        <th>MMStar</th>
                                        <th>MME</th>
                                        <th>OCRBench</th>
                                        <th>MMB-en</th>
                                        <th>MMB-cn</th>
                                        <th>TextVQA</th>
                                        <th>DocVQA</th>
                                        <th>InfoVQA</th>
                                        <th>AI2D</th>
                                        <th>ChartQA</th>
                                        <th>RealWorldQA</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Prismatic VLM</strong></td>
                                        <td>7b</td>
                                        <td>35.0</td>
                                        <td>38.8</td>
                                        <td><strong>1456.6</strong></td>
                                        <td>32.0</td>
                                        <td>66.2</td>
                                        <td>55.7</td>
                                        <td>42.5</td>
                                        <td>17.5</td>
                                        <td>19.7</td>
                                        <td>54.6</td>
                                        <td>16.7</td>
                                        <td>30.8</td>
                                    </tr>
                                    <tr>
                                        <td><strong>OpenVLA</strong></td>
                                        <td>7b</td>
                                        <td>26.3</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>43.0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                    </tr>
                                    <tr>
                                        <td><strong>ECoT</strong></td>
                                        <td>7b</td>
                                        <td>26.6</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0.01</td>
                                        <td>3.7</td>
                                        <td>4.1</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>0</td>
                                        <td>25.6</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Gemma-3-4b-it</strong></td>
                                        <td>4b</td>
                                        <td>39.3</td>
                                        <td>37.1</td>
                                        <td>1205.8</td>
                                        <td><u>70.2</u></td>
                                        <td><u>68.6</u></td>
                                        <td>64.3</td>
                                        <td>61.5</td>
                                        <td>68.8</td>
                                        <td>40.9</td>
                                        <td>70.5</td>
                                        <td>50.3</td>
                                        <td><u>44.0</u></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Gemma-3-12b-it</strong></td>
                                        <td>12b</td>
                                        <td><strong>46.0</strong></td>
                                        <td><u>46.3</u></td>
                                        <td>1182.3</td>
                                        <td><strong>75.0</strong></td>
                                        <td><strong>76.9</strong></td>
                                        <td><strong>74.7</strong></td>
                                        <td><strong>68.9</strong></td>
                                        <td><strong>80.6</strong></td>
                                        <td><strong>50.4</strong></td>
                                        <td><strong>78.5</strong></td>
                                        <td><u>55.1</u></td>
                                        <td><strong>50.6</strong></td>
                                    </tr>
                                    <tr style="background-color: #f2f2f2;">
                                        <td><strong>Ours</strong></td>
                                        <td>12b</td>
                                        <td><u>42.7</u></td>
                                        <td><strong>48.0</strong></td>
                                        <td><u>1391.7</u></td>
                                        <td>63.9</td>
                                        <td>68.5</td>
                                        <td><u>67.6</u></td>
                                        <td><u>64.9</u></td>
                                        <td><u>78.4</u></td>
                                        <td><u>46.2</u></td>
                                        <td><u>74.0</u></td>
                                        <td><strong>58.3</strong></td>
                                        <td>43.3</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <!-- Right Column (4/12 width): The Image -->
                    <div class="col-md-4 align-self-center">
                        <img src="img/vqa.png" class="img-fluid" alt="Radar plot of <b>VLM2VLA</b> against the base VLM backbone.">
                        <p class="text-center mt-2"><em>Qualitative examples of VQA performance. <span style="color:blue;">Blue</span> is Gemma-3-12B-IT, <span style="color:purple;">purple</span> is <b>VLM2VLA</b></em></p>
                    </div>

                </div>

            </div>
        </section>
        <!-- ================================================================================= -->
        <!-- END MULTIMODAL UNDERSTANDING EVALUATION SECTION -->
        <!-- ================================================================================= -->
         
        <!-- ================================================================================= -->
        <!-- START REAL ROBOT EXPERIMENTS SECTION -->
        <!-- ================================================================================= -->

        <!-- This CSS provides the styling for the carousel -->
        <style>
            .carousel-container {
                position: relative;
                width: 100%;
                max-width: 720px; /* Adjust this to control the max width of your video */
                margin: auto;
            }
            .carousel-slide {
                display: none; /* Hide all slides by default */
                text-align: center;
            }
            .carousel-slide.active-slide {
                display: block; /* Show only the active slide */
            }
            .video-item {
                width: 100%;
                border-radius: 15px;
                box-shadow: 0 4px 12px rgba(0,0,0,0.15);
                margin-top: 10px;
            }
            /* Styling for the next/previous arrows */
            .carousel-arrow {
                position: absolute;
                top: 55%; /* Vertically center the arrows on the video */
                transform: translateY(-50%);
                background-color: rgba(0, 0, 0, 0.4);
                color: white;
                border: none;
                border-radius: 50%;
                width: 40px;
                height: 40px;
                font-size: 20px;
                cursor: pointer;
                z-index: 10;
                display: flex;
                justify-content: center;
                align-items: center;
            }
            .carousel-arrow:hover {
                background-color: rgba(0, 0, 0, 0.7);
            }
            .carousel-arrow.prev {
                left: 15px;
            }
            .carousel-arrow.next {
                right: 15px;
            }
        </style>

        <section class="section">
            <div class="container is-max-desktop">

                <!-- Main Section Title and Description -->
                <div class="row justify-content-md-center">
                    <div class="col-md-12">
                        <h2 class="title is-3" style="text-align: center;">Real Robot Experiments</h2>
                        <p class="text-justify" style="text-align: center; max-width: 800px; margin: auto;">
                            We showcase our model's capabilities on numerous manipulation tasks, some requiring multilingual understanding, object generalization, and following complex, open-vocabulary instructions. We contrast our method to two state-of-the-art VLAs: OpenVLA and Embodied Chain of Thought (ECoT). Videos are shown below.
                        </p>
                    </div>
                </div>
                <br>

                <!-- ROW 1: THE VIDEO CAROUSEL -->
                <div class="row justify-content-md-center">
                    <div class="col-md-12">
                        <div class="carousel-container">
                            <!-- Slide 1 -->
                            <div class="carousel-slide">
                                <h4 class="title is-5">Pick up the Carrot</h4>
                                <video class="video-item" autoplay loop muted playsinline src="videos/pickupcarrot.mp4"></video>
                            </div>
                            <!-- Slide 2 -->
                            <div class="carousel-slide">
                                <h4 class="title is-5">Pick up the item on Ash Ketchum</h4>
                                <video class="video-item" autoplay loop muted playsinline src="videos/pickupitemonash.mp4"></video>
                            </div>
                            <!-- Slide 3 -->
                            <div class="carousel-slide">
                                <h4 class="title is-5">Pick up the Carrot (Instruction in Hindi)</h4>
                                <video class="video-item" autoplay loop muted playsinline src="videos/hindi.mp4"></video>
                            </div>
                            <!-- Slide 4 -->
                            <div class="carousel-slide">
                                <h4 class="title is-5">Pick up the Carrot (Instruction in Mandarin)</h4>
                                <video class="video-item" autoplay loop muted playsinline src="videos/mandarin.mp4"></video>
                            </div>
                            
                            <!-- Navigation Arrows -->
                            <button class="carousel-arrow prev" onclick="changeSlide(-1)">&#10094;</button>
                            <button class="carousel-arrow next" onclick="changeSlide(1)">&#10095;</button>
                        </div>
                    </div>
                </div>
                <br>

                <!-- ROW 2: THE STATIC GRAPH (WITH CORRECTED WIDTH) -->
                <div class="row justify-content-md-center">
                    <!-- === CHANGED HERE === -->
                    <div class="col-md-12 col-lg-10">
                        <p style="text-align:center;">
                            <img src="img/model_comparison_generalization_types_OOD.png" width="100%" alt="Graph comparing VLM2VLA performance on in-distribution and out-of-distribution tasks against OpenVLA and ECoT.">
                        </p>
                        <br>
                        <p class="text-justify">
                            Comparative evaluation of VLA performance on in-distribution (ID) and out-of-distribution (OOD) robotic
                            manipulation tasks. <strong>VLM2VLA</strong> maintains high success rates on OOD tasks, highlighting its superior generalization
                            capabilities. Embodied Chain of Thought (ECoT) and <strong>VLM2VLA</strong> were fine-tuned on the Bridgev2 dataset, whereas OpenVLA
                            was fine-tuned on the Open-X-Embodiment dataset.
                        </p>
                    </div>
                </div>

            </div>
        </section>

        <!-- This JavaScript provides the functionality for the carousel -->
        <script>
            // Set the default slide to be "Pick up the item on Ash Ketchum", which is the 2nd video (index 1)
            let currentSlideIndex = 1;
            
            // Get all the slide elements from the page
            const slides = document.querySelectorAll(".carousel-slide");

            // Function to show a specific slide
            function showSlide(index) {
                // Hide all slides first
                slides.forEach(slide => {
                    slide.classList.remove('active-slide');
                });
                // Show the requested slide
                slides[index].classList.add('active-slide');
            }

            // Function to change the slide when an arrow is clicked
            function changeSlide(direction) {
                // Update the index
                currentSlideIndex += direction;

                // Wrap around if we go past the end or before the beginning
                if (currentSlideIndex >= slides.length) {
                    currentSlideIndex = 0;
                } else if (currentSlideIndex < 0) {
                    currentSlideIndex = slides.length - 1;
                }
                
                // Show the new slide
                showSlide(currentSlideIndex);
            }

            // Initialize the carousel to show the default slide when the page loads
            document.addEventListener("DOMContentLoaded", function() {
                showSlide(currentSlideIndex);
            });
        </script>

        <!-- ================================================================================= -->
        <!-- END REAL ROBOT EXPERIMENTS SECTION -->
        <!-- ================================================================================= -->

        <!-- <div class='myscope'>
            <section class="hero is-light is-small">
              <div class="hero-body">
                <div class="container">
                  <div id="results-carousel" class="carousel results-carousel">
                    <div class="item-1">
                      <video poster="" id="1" controls muted loop playsinline height="100%">
                          <source src="videos/4x/octo/octobase_noobjects_success_4x.mp4" type="video/mp4">
                      </video>
                      <div class="video-overlay">
                          <p>
                            Task: "place the carrot on yellow plate"<br>
                            Scenario: no distractors <br>
                            Policy: Octo-Base
                          </p>
                          <p style="color: green">Success</p>
                      </div>
                    </div>
                    <div class="item-2">
                        <video poster="" id="2" controls muted loop playsinline height="100%">
                            <source src="videos/4x/octo/octobase_objects_failure_4x.mp4" type="video/mp4">
                        </video>
                        <div class="video-overlay">
                            <p>
                              Task: "place the carrot on yellow plate"<br>
                              Scenario: object distractors <br>
                              Policy: Octo-Base
                            </p>
                            <p style="color: red">Failure</p>
                        </div>
                    </div>
                    <div class="item-3">
                        <video poster="" id="3" controls muted loop playsinline height="100%">
                            <source src="videos/4x/octo/octobyovla_objects_success_4x.mp4" type="video/mp4">
                        </video>
                        <div class="video-overlay">
                            <p>
                              Task: "place the carrot on yellow plate"<br>
                              Scenario: object distractors <br>
                              Policy: Octo-Base <b>with BYOVLA</b>
                            </p>
                            <p style="color: green">Success</p>
                        </div>
                    </div>
                    <div class="item-4">
                        <video poster="" id="4" controls muted loop playsinline height="100%">
                            <source src="videos/4x/openvla/openvla_nodistractions_success_4x.mp4" type="video/mp4">
                        </video>
                        <div class="video-overlay">
                            <p>
                              Task: "place the eggplant in the pot"<br>
                              Scenario: no distractors <br>
                              Policy: OpenVLA-7b
                            </p>
                            <p style="color: green">Success</p>
                        </div>
                      </div>
                      <div class="item-5">
                          <video poster="" id="5" controls muted loop playsinline height="100%">
                              <source src="videos/4x/openvla/openvla_distractions_failure_4x.mp4" type="video/mp4">
                          </video>
                          <div class="video-overlay">
                              <p>
                                Task: "place the eggplant in the pot"<br>
                                Scenario: object distractors <br>
                                Policy: OpenVLA-7b
                              </p>
                              <p style="color: red">Failure</p>
                          </div>
                      </div>
                      <div class="item-6">
                          <video poster="" id="6" controls muted loop playsinline height="100%">
                              <source src="videos/4x/openvla/openvlabyovla_distractions_success_4x.mp4" type="video/mp4">
                          </video>
                          <div class="video-overlay">
                              <p>
                                Task: "place the eggplant in the pan"<br>
                                Scenario: object distractors <br>
                                Policy: OpenVLA-7b <b>with BYOVLA</b>
                              </p>
                              <p style="color: green">Success</p>
                          </div>
                      </div>
                      <div class="item-7">
                        <video poster="" id="6" controls muted loop playsinline height="100%">
                            <source src="videos/4x/octo/octobase_backgrounds_failure_4x.mp4" type="video/mp4">
                        </video>
                        <div class="video-overlay">
                            <p>
                              Task: "place the carrot on yellow plate"<br>
                              Scenario: background distractors <br>
                              Policy: Octo-Base
                            </p>
                            <p style="color: red">Failure</p>
                        </div>
                       </div>
                       <div class="item-8">
                        <video poster="" id="6" controls muted loop playsinline height="100%">
                            <source src="videos/4x/octo/octobyovla_backgrounds_success_4x.mp4" type="video/mp4">
                        </video>
                        <div class="video-overlay">
                            <p>
                              Task: "place the eggplant in the pot"<br>
                              Scenario: background distractors <br>
                              Policy: Octo-Base <b>with BYOVLA</b>
                            </p>
                            <p style="color: green">Success</p>
                        </div>
                    </div>
                  </div>
                </div>
              </div>
            </section>
        </div>
        <br> -->

        <!-- <div class="row justify-content-md-center">
            <div class="col-md-10 col-lg-8">
            <p style="text-align:center;">
            </p>
            <h2 class="title is-3">Results</h2>
                <p class="text-justify">
                    We evaluate BYOVLA on Octo-Base with the task "place the carrot on yellow plate." With distractions, the performance of the nominal VLA policy significantly drops. Augmenting Octo-Base with <b>BYOVLA allows the policy to achieve its nominal task-success rate </b> with distractions present. 
                </p>
         
            <br>
            <p style="text-align:center;">
                <image src="img/octo_figure.png" width="100%"></image>
            </p>
            <br>
            <p class="text-justify">
                We also evaluate BYOVLA on OpenVLA-7b with the task "put the eggplant in the pot." Even though OpenVLA is 75x larger than Octo-Base, task-irrelevant distractions markedly degrade performance. We find that application of BYOVLA helps OpenVLA achieve its nominal task-success rate with distractions present. 
            </p>
            <br>
            <p style="text-align:center;">
                <image src="img/openVLA_results.png" width="100%"></image>
            </p>
        
        </div>

    
        <br> -->

        <!--
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{hancock2024byovla,
                                title={Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust},
                                author={Hancock, Asher J., and Ren, Allen Z., and Majumdar, Anirudha}
                                journal={arXiv preprint FILL OUT LATER},
                                year={2024},}
            </code></pre>
        </div>
        -->

    <!---->
    <!-- <div class="row justify-content-md-center">
        <div class="col-md-10 col-lg-8">
        <p style="text-align:center;">
        </p>
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{hancock24byovla,
      title={Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust},
      author={Hancock, Asher J., and Ren, Allen Z., and Majumdar, Anirudha},
      journal = {arXiv preprint arXiv:2410.01971},
      year={2024},
  } </code></pre>
          </div>
          </div>
          </div>

    </div> -->
</body>
</html>
